# Data Engineering project using Azure ğŸš€

This document outlines a data engineering project that utilizes Microsoft Azure for comprehensive data analytics. The project is divided into several key steps:

## 1. Data Download from Kaggle ğŸ“Š
The primary data source for this project is Kaggle, a popular online platform that hosts various datasets. The initial step involves downloading the necessary data from this platform.

## 2. Ingestion to Azure Data Factory ğŸ› ï¸
The downloaded data is then ingested into Azure Data Factory, a cloud-based data integration service that allows the creation of data-driven workflows for orchestrating and automating data movement and data transformation.

## 3. Storage of Raw Data in Data Lake Gen 2 ğŸ—„ï¸
Once ingested, the raw data is stored in Azure Data Lake Gen 2. This service combines the scalability and cost benefits of object storage with the reliability and performance of the Big Data file system capabilities.

## 4. ETL Processing in Azure Databricks ğŸ“ˆ
The raw data undergoes Extract, Transform, Load (ETL) operations in Azure Databricks, utilizing PySpark for the processing.

## 5. Storage of Transformed Data in Data Lake Gen 2 ğŸ”„
The transformed data is then stored back in Azure Data Lake Gen 2 for further analysis.

## 6. Visualization with Power BI ğŸ“Š
Finally, the insights derived from the analysis are visualized using Power BI, a business analytics tool that delivers interactive visualizations with self-service business intelligence capabilities.

Each step in this roadmap not only showcases the capabilities of Azure's data engineering services but also mirrors the typical workflow of a data engineering project.

